{"cells":[{"cell_type":"code","source":["#Configuração do container e keys\nSTORAGE_ACCOUNT_NAME = 'nomedastorageaccount'\nCONTAINER_INPUT = 'nomecontainerinput'\nCONTAINER_OUTPUT = 'nomecontaineroutput'\nKEY = ''\n\n#Mount do diretório\nif not any(mount.mountPoint == '/mnt/input' for mount in dbutils.fs.mounts()):\n  dbutils.fs.mount(\n     source = \"wasbs://{}@{}.blob.core.windows.net\".format(CONTAINER_INPUT, STORAGE_ACCOUNT_NAME),\n     mount_point = \"/mnt/{}\".format(CONTAINER_INPUT),\n     extra_configs = {\"fs.azure.account.key.{}.blob.core.windows.net\".format(STORAGE_ACCOUNT_NAME):\"{}\".format(KEY)})"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["# Montando o Dataframe\nFILENAME = \"/pasta/arquivo.csv\".format(CONTAINER_INPUT)\n\ndf = (spark\n  .read\n  .option(\"header\", True)\n  .option(\"header\", True) \\\n  .option(\"sep\", ',') \\\n  .csv(FILENAME)\n)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["#Importando as bibliotecas\nfrom __future__ import print_function\nimport requests\nimport json\nimport pandas as pd\nimport numpy as np\nimport warnings\nimport matplotlib\nwarnings.filterwarnings('ignore')\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom bokeh.resources import CDN\nfrom bokeh.embed import components, file_html\nfrom bokeh.plotting import figure,output_notebook, show\nfrom bokeh.palettes import Blues4\nfrom bokeh.models import ColumnDataSource,Slider\nimport datetime\nfrom bokeh.io import push_notebook\nfrom dateutil import parser\nfrom ipywidgets import interact, widgets, fixed\noutput_notebook()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["#CONFIGURANDO API ANOMALIA\n# To start sending requests to the Anomaly Detector API, paste your subscription key you received after creating Anomaly Detector resource. \nsubscription_key = 'subscriptionkey' \n\n# Use the endpoint your received from overview section of the Anomaly Detector resource you created\n# the endpoint is like https://westus2.api.cognitive.microsoft.com/, different by regions, you need to concat anomalydetector/v1.0/timeseries/entire/detect\n\nendpoint = 'endpoint'"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["#Função que chama a API\ndef detect(endpoint, subscription_key, request_data):\n    #print(request_data)\n    headers = {'Content-Type': 'application/json', 'Ocp-Apim-Subscription-Key': subscription_key}\n    response = requests.post(endpoint, data=json.dumps(request_data), headers=headers)\n    if response.status_code == 200:\n        return json.loads(response.content.decode(\"utf-8\"))\n    else:\n        print(response.status_code)\n        raise Exception(response.text)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# Definindo função que trata o retorno da API\ndef build_figure(sample_data, sensitivity):\n    sample_data['sensitivity'] = sensitivity\n    # Chama a função detect que chama a API\n    result = detect(endpoint, subscription_key, sample_data)    \n    columns = {'expectedValues': result['expectedValues'], 'isAnomaly': result['isAnomaly'], 'isNegativeAnomaly': result['isNegativeAnomaly'],\n          'isPositiveAnomaly': result['isPositiveAnomaly'], 'upperMargins': result['upperMargins'], 'lowerMargins': result['lowerMargins'],\n          'timestamp': [parser.parse(x['timestamp']) for x in sample_data['series']], \n          'value': [x['value'] for x in sample_data['series']]}\n    response = pd.DataFrame(data=columns)\n    values = response['value']\n    label = response['timestamp']\n    anomalies = []\n    anomaly_labels = []\n    index = 0\n    anomaly_indexes = []\n    p = figure(x_axis_type='datetime', title=\"Batch Anomaly Detection ({0} Sensitvity)\".format(sensitivity), width=700, height=600)\n    for anom in response['isAnomaly']:\n       \n        if anom == True:\n          current_value = int(values[index])\n          iloc_expvalue = response.iloc[index]['expectedValues']\n          iloc_upperMargins = response.iloc[index]['upperMargins']\n          iloc_lowerMargins = response.iloc[index]['lowerMargins']\n          sum_exp_upper = iloc_expvalue + iloc_upperMargins\n          dif_exp_lower = iloc_expvalue - iloc_lowerMargins\n          current_value_greaterthan_sum_exp_upper = current_value > sum_exp_upper\n          current_value_lowerthan_sum_exp_upper = current_value < dif_exp_lower\n          \n          if (current_value_greaterthan_sum_exp_upper or current_value_lowerthan_sum_exp_upper):           \n            anomalies.append(values[index])\n            anomaly_labels.append(label[index])\n            anomaly_indexes.append(index)\n        \n        index = index+1\n        \n    upperband = response['expectedValues'] + response['upperMargins']\n    lowerband = response['expectedValues'] -response['lowerMargins']\n    band_x = np.append(label, label[::-1])\n    band_y = np.append(lowerband, upperband[::-1])\n    boundary = p.patch(band_x, band_y, color=Blues4[2], fill_alpha=0.5, line_width=1, legend_label='Boundary')\n    p.line(label, values, legend_label='Value', color=\"#2222aa\", line_width=1)\n    p.line(label, response['expectedValues'], legend_label='ExpectedValue',  line_width=1, line_dash=\"dotdash\", line_color='olivedrab')\n    anom_source = ColumnDataSource(dict(x=anomaly_labels, y=anomalies))\n    anoms = p.circle('x', 'y', size=5, color='tomato', source=anom_source)\n    p.legend.border_line_width = 1\n    p.legend.background_fill_alpha  = 0.1\n    # cria um html para fazer o plot da Bokeh\n    html = file_html(p, CDN, \"my plot1\")\n    # faz o display html\n    displayHTML(html)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["from pyspark.sql.functions import col, udf, date_format, asc\nfrom pyspark.sql.types import DateType, TimestampType \nfrom datetime import datetime\n\n# Cria função para converter a data em um timestamp\nfunc =  udf (lambda x: datetime.strptime(x, '%m/%d/%Y %H:%M:%S'), TimestampType())\ndf_dt = df.withColumn('timestamp', func(col('TimeStampScript')))\n\n# Converte no formato de data ISO-8601 yyyy-MM-dd'T'HH:mm:ss'Z' criando a coluna timestamp\ndf_dt = df_dt.withColumn(\"timestamp\", date_format(col(\"timestamp\"), \"yyyy-MM-dd'T'HH:mm:ss'Z'\"))\n\n# Cria a coluna value\ndf_dt = df_dt.withColumn('value', col('IOOtherBytesPersec'))"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# Faz o filtro pela coluna escolhida no csv\nfilter = df_dt[\"Name\"]==\"OUTLOOK\"\n\n# Filtra os dados do dataframe pegando value e timestamp filtrando pelo outlook\ndf_anomaly = df_dt.select('value', df_dt.timestamp).where(filter).sort(asc(\"timestamp\"))\n\n# Converte o dataframe para dicionario\ndic_time_series = df_anomaly.rdd.map(lambda row: row.asDict()).collect()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# Estrutura de dados que vai para a API\nsample_data = {}\nsample_data[\"series\"] = dic_time_series\nsample_data['granularity'] = 'minutely'\nsample_data['customInterval'] = 6\n\n# 85 sensitivity\nbuild_figure(sample_data,85)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["#Resultado API sem o gráfico\n\nsample_data['sensitivity'] = 85\nresult = detect(endpoint, subscription_key, sample_data)\nresult"],"metadata":{},"outputs":[],"execution_count":10}],"metadata":{"name":"anomalia_detect","notebookId":3062100559076573},"nbformat":4,"nbformat_minor":0}
